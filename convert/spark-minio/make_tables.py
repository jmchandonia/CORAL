"""make_tables.py

This file is auto‑generated by ``to_spark.py``.  It contains a
``generate_cdm(spark)`` function that builds the CDM tables in a Spark
session (Delta Lake format).  The script follows the conventions used in
``update_coral_ontologies.py`` and adds robust handling for the
``scalar_type == "term"`` fields.

Key behaviours
--------------
* System types → tables named ``sys_<type>`` (e.g. ``sys_process``)
* Static types → tables named ``sdt_<type>`` (e.g. ``sdt_genome``)
* Primary‑key column → ``<table>_id``
* Foreign‑key column → ``<referenced_table>_id`` (or ``_ids`` for arrays)
* ``scalar_type == "term"`` fields are **expanded in place** into two
  columns:
  ``<field>_sys_oterm_id`` (FK → ``sys_oterm``) and
  ``<field>_sys_oterm_name`` (the term name)
* All column names are lower‑case snake_case (underscores only)
* The TSV loader expects a *single* combined column for each term field of
  the form ``<term name> <term id>`` – e.g. ``Assay Growth <PROCESS:0000006>``
  – and splits it back into the two generated columns.
* Missing columns in a TSV are added as ``NULL`` so that the final schema
  can always be satisfied.
* Before each table is written we log its schema and the first few rows
  (up to 5) for debugging.
* Each table is dropped first (``DROP TABLE IF EXISTS``) to guarantee a
  clean creation.

Run the script in a notebook as follows::

    spark = get_spark_session()               # <-- defined elsewhere
    db_name = "jmc_coral"
    spark.sql(f"USE {db_name}")
    generate_cdm(spark, db_name)
"""

import json
import logging
import os
import re
from typing import Any, Dict, List, Tuple

from pyspark.sql import SparkSession, functions as F
from pyspark.sql.types import (
    StructType,
    StructField,
    StringType,
    IntegerType,
    DoubleType,
    BooleanType,
    ArrayType,
    DataType,
)

# ------------------------------------------------------------------
# Helper: parse an ``s3a://`` URI into bucket + prefix
# ------------------------------------------------------------------
def _parse_s3a_uri(s3a_uri: str) -> Tuple[str, str]:
    """Return (bucket, prefix) from a ``s3a://`` URI."""
    if not s3a_uri.startswith("s3a://"):
        raise ValueError(f"Unexpected S3A URI: {s3a_uri}")

    path = s3a_uri[len("s3a://") :]          # e.g. "cdm‑lake/users‑general‑warehouse/jmc"
    parts = path.split("/", 1)
    bucket = parts[0]
    prefix = parts[1].rstrip("/") + "/" if len(parts) == 2 else ""
    return bucket, prefix


# ------------------------------------------------------------------
# Normalise column names (lower‑case, underscores only)
# ------------------------------------------------------------------
def _normalize_name(name: str) -> str:
    """Convert an arbitrary name to snake_case."""
    name = name.strip()
    name = re.sub(r"[ \-]+", "_", name)            # spaces / hyphens → _
    name = re.sub(r"[^0-9a-zA-Z_]", "_", name)     # everything else → _
    name = re.sub(r"_+", "_", name)                # collapse multiple _
    return name.lower().strip("_")


# ------------------------------------------------------------------
# CDM naming helpers
# ------------------------------------------------------------------
def map_scalar_type(scalar_type: str) -> DataType:
    """Map a CORAL ``scalar_type`` to a Spark DataType."""
    if scalar_type.startswith("[") and scalar_type.endswith("]"):
        inner = scalar_type[1:-1].strip()
        return ArrayType(map_scalar_type(inner), containsNull=False)

    st = scalar_type.lower()
    if st in ("text", "term"):
        return StringType()
    if st == "int":
        return IntegerType()
    if st in ("float", "double"):
        return DoubleType()
    if st == "bool":
        return BooleanType()
    return StringType()


def parse_fk(fk_str: str) -> Tuple[str, bool]:
    """Parse a foreign‑key descriptor, returning (type, is_array)."""
    is_array = False
    if fk_str.startswith("[") and fk_str.endswith("]"):
        is_array = True
        fk_str = fk_str[1:-1].strip()
    return fk_str.split(".", 1)[0], is_array


def rename_fk_column(original_name: str, ref_type: str, is_array: bool,
                    type_to_table: Dict[str, str]) -> str:
    """
    Rename a foreign‑key column according to CDM conventions and then
    normalise it to snake_case.

    * If the original column name contains the referenced type (singular or
      plural), replace that part with the CDM table name and insert ``_id``
      or ``_ids`` **before** any remaining suffix.
      Example: ``genes_changed`` → ``sdt_gene_ids_changed``.
    * If it does not contain the referenced type, append ``_<cdm_table>_id``
      (or ``_ids``) to the original name.
      Example: ``derived_from`` → ``derived_from_sdt_genome_id``.
    """
    target_table = type_to_table.get(ref_type, ref_type.lower())
    suffix = "_ids" if is_array else "_id"

    lower_orig = original_name.lower()
    ref_singular = ref_type.lower()
    ref_plural = ref_singular + "s"

    idx = -1
    match_len = 0
    if ref_plural in lower_orig:
        idx = lower_orig.find(ref_plural)
        match_len = len(ref_plural)
    elif ref_singular in lower_orig:
        idx = lower_orig.find(ref_singular)
        match_len = len(ref_singular)

    if idx != -1:
        prefix = lower_orig[:idx]
        suffix_part = lower_orig[idx + match_len :]   # keep whatever follows the match
        new_name = f"{prefix}{target_table}{suffix}{suffix_part}"
    else:
        new_name = f"{lower_orig}_{target_table}{suffix}"

    return _normalize_name(new_name)


def field_to_column_name(field: Dict[str, Any],
                         current_table: str,
                         type_to_table: Dict[str, str]) -> str:
    """
    Derive the CDM column name for a field (non‑term fields only) and
    normalise it.
    """
    if field.get("PK", False):
        return _normalize_name(f"{current_table}_id")
    if field.get("UPK", False):
        return "name"                     # will be renamed later to <table>_name
    fk = field.get("FK")
    if fk:
        ref_type, is_array = parse_fk(fk)
        return rename_fk_column(field["name"], ref_type, is_array, type_to_table)
    return _normalize_name(field["name"])


# ------------------------------------------------------------------
# Process a single field – may produce one or two CDM columns
# ------------------------------------------------------------------
def process_field(field: Dict[str, Any],
                  type_name: str,
                  current_table: str,
                  type_to_table: Dict[str, str]) -> Tuple[
                      List[StructField],
                      List[Dict[str, Any]],
                      List[Dict[str, Any]]
                  ]:
    """
    Convert a CORAL field definition into Spark ``StructField`` objects,
    rows for the ``sys_typedef`` table, and (if needed) term‑mapping
    dictionaries used later to split the combined TSV column.
    """
    def build(col_name: str,
              spark_type: DataType,
              nullable: bool,
              comment: str,
              fk: str = None) -> Tuple[StructField, Dict[str, Any]]:
        metadata = {
            "orig_name": field.get("name"),
            "type_term": field.get("type_term"),
            "required": field.get("required", False),
            "pk": field.get("PK", False),
            "upk": field.get("UPK", False),
            "comment": comment,
        }
        if "constraint" in field:
            metadata["constraint"] = (
                json.dumps(field["constraint"])
                if isinstance(field["constraint"], (list, dict))
                else field["constraint"]
            )
        if "units_term" in field:
            metadata["units_term"] = field["units_term"]
        if fk:
            metadata["fk"] = fk

        struct = StructField(col_name, spark_type, nullable=nullable, metadata=metadata)

        typedef = {
            "type_name": type_name,
            "field_name": field.get("name"),
            "cdm_column_name": col_name,
            "scalar_type": field.get("scalar_type", "text"),
            "required": field.get("required", False),
            "pk": field.get("PK", False),
            "upk": field.get("UPK", False),
            "fk": fk,
            "constraint": field.get("constraint"),
            "comment": comment,
            "units_term": field.get("units_term"),
            "type_term": field.get("type_term"),
        }
        return struct, typedef

    # ------------------------------------------------------------------
    # TERM fields – split into id + name columns (keep original order)
    # ------------------------------------------------------------------
    if field.get("scalar_type") == "term":
        base = _normalize_name(field.get("name"))
        id_col = f"{base}_sys_oterm_id"
        name_col = f"{base}_sys_oterm_name"

        id_comment = f"Foreign key to `sys_oterm` (term id for field `{field.get('name')}`)"
        name_comment = f"Term name for field `{field.get('name')}`"

        id_field, id_typedef = build(
            id_col,
            StringType(),
            not field.get("required", False),
            id_comment,
            fk="sys_oterm.id",
        )
        name_field, name_typedef = build(
            name_col,
            StringType(),
            not field.get("required", False),
            name_comment,
        )

        term_mapping = {
            "orig_name": field.get("name"),   # column name as it appears in the TSV
            "id_col": id_col,
            "name_col": name_col,
            "required": field.get("required", False),
        }
        return [id_field, name_field], [id_typedef, name_typedef], [term_mapping]

    # ------------------------------------------------------------------
    # NON‑TERM fields – regular handling
    # ------------------------------------------------------------------
    col_name = field_to_column_name(field, current_table, type_to_table)
    spark_type = map_scalar_type(field.get("scalar_type", "text"))
    nullable = not field.get("required", False)

    if field.get("PK", False):
        comment = f"Primary key for table `{current_table}`"
    elif field.get("FK"):
        ref_type, _ = parse_fk(field["FK"])
        comment = f"Foreign key to `{type_to_table.get(ref_type, ref_type.lower())}`"
    else:
        comment = field.get("comment") or f"Field `{field.get('name')}`"

    struct, typedef = build(col_name, spark_type, nullable, comment,
                           fk=field.get("FK") if field.get("FK") else None)
    return [struct], [typedef], []


# ------------------------------------------------------------------
# Build the full schema for a type, collecting term‑split info
# ------------------------------------------------------------------
def generate_schema(type_def: Dict[str, Any],
                    table_name: str,
                    type_to_table: Dict[str, str]) -> Tuple[
                        StructType,
                        List[Dict[str, Any]],
                        List[Dict[str, Any]]
                    ]:
    """
    Return (schema, typedef_rows, term_mappings) for a CORAL type.
    The schema preserves the order of fields, expanding term fields into the
    two generated columns in the same position as the original field.
    """
    struct_fields: List[StructField] = []
    typedef_rows: List[Dict[str, Any]] = []
    term_mappings: List[Dict[str, Any]] = []

    for f in type_def.get("fields", []):
        fields, rows, terms = process_field(f, type_def.get("name"), table_name, type_to_table)
        struct_fields.extend(fields)
        typedef_rows.extend(rows)
        term_mappings.extend(terms)

    # Ensure a primary‑key column exists (if not already added by a PK field)
    pk_name = f"{table_name}_id"
    if not any(sf.name == pk_name for sf in struct_fields):
        struct_fields.insert(
            0,
            StructField(
                pk_name,
                StringType(),
                nullable=False,
                metadata={"comment": f"Primary key for table `{table_name}`"},
            ),
        )

    # Add a ``name`` column for user‑primary‑key (UPK) if needed
    if any(r["upk"] for r in typedef_rows) and not any(sf.name == "name" for sf in struct_fields):
        struct_fields.append(
            StructField(
                "name",
                StringType(),
                nullable=False,
                metadata={"comment": "User‑defined primary key (UPK)"},
            )
        )

    return StructType(struct_fields), typedef_rows, term_mappings


# ------------------------------------------------------------------
# Build the auxiliary ``sys_typedef`` DataFrame
# ------------------------------------------------------------------
def build_typedefs_dataframe(spark, typedef_rows: List[Dict[str, Any]]):
    schema = StructType([
        StructField("type_name", StringType(), nullable=False),
        StructField("field_name", StringType(), nullable=False),
        StructField("cdm_column_name", StringType(), nullable=False),
        StructField("scalar_type", StringType(), nullable=True),
        StructField("required", BooleanType(), nullable=True),
        StructField("pk", BooleanType(), nullable=True),
        StructField("upk", BooleanType(), nullable=True),
        StructField("fk", StringType(), nullable=True),
        StructField("constraint", StringType(), nullable=True),
        StructField("comment", StringType(), nullable=True),
        StructField("units_term", StringType(), nullable=True),
        StructField("type_term", StringType(), nullable=True),
    ])

    rows = [
        (
            r["type_name"],
            r["field_name"],
            r["cdm_column_name"],
            r["scalar_type"],
            r["required"],
            r["pk"],
            r["upk"],
            r["fk"],
            json.dumps(r["constraint"])
            if isinstance(r["constraint"], (list, dict))
            else r["constraint"],
            r["comment"],
            r["units_term"],
            r["type_term"],
        )
        for r in typedef_rows
    ]
    return spark.createDataFrame(rows, schema)


# ------------------------------------------------------------------
# Helper: convert a stringified array back to an actual ArrayType column
# ------------------------------------------------------------------
def _convert_array_column(col_name: str) -> Any:
    """
    Turn a TSV string like ``[a,b,c]`` into an ``ArrayType(String)``.
    Empty or null values become an empty array.
    """
    cleaned = F.regexp_replace(F.col(col_name), r"^\[|\]$", "")
    splitted = F.split(cleaned, r",\s*")
    without_empty = F.array_remove(splitted, "")
    return F.when(F.col(col_name).isNull() | (F.col(col_name) == ""), F.array()).otherwise(without_empty)


# ------------------------------------------------------------------
# Helper: split a combined term column ``"<name> <id>"`` into two columns
# ------------------------------------------------------------------
def _split_term_column(df,
                       orig_name: str,
                       id_col: str,
                       name_col: str):
    """
    From a column that contains ``term name <term id>`` (or only one part)
    create two new columns: ``id_col`` and ``name_col`` and drop the
    original column.
    """
    term_str = F.col(orig_name).cast("string")

    # ---- extract the ID (text inside <...>) ---------------------------------
    raw_id = F.regexp_extract(term_str, r"<([^>]+)>", 1)          # empty string if no match
    id_extracted = F.when(raw_id == "", None).otherwise(raw_id)

    # ---- extract the name (everything before the angle‑bracket part) -------
    raw_name = F.regexp_extract(term_str, r"^(.*?)(?:\s*<[^>]+>)?\s*$", 1)
    name_extracted = F.when(raw_name == "", None).otherwise(F.trim(raw_name))

    # ---- handle completely empty cells --------------------------------------
    id_extracted = F.when(term_str.isNull() | (term_str == ""), None).otherwise(id_extracted)
    name_extracted = F.when(term_str.isNull() | (term_str == ""), None).otherwise(name_extracted)

    df = df.withColumn(id_col, id_extracted).withColumn(name_col, name_extracted)
    df = df.drop(orig_name)
    return df


# ------------------------------------------------------------------
# Main entry point – to be called from a notebook
# ------------------------------------------------------------------
def generate_cdm(spark,
                 db_name: str = "jmc_coral",
                 load_tsv: bool = True,
                 fmt: str = "delta"):
    """
    Build the CDM tables.

    * System types → ``sys_<type>``
    * Static types → ``sdt_<type>``
    * ``term`` fields become ``<field>_sys_oterm_id`` and
      ``<field>_sys_oterm_name`` in the same position as the original column.
    * All column names are lower‑case snake_case.
    """
    # ------------------------------------------------------------------
    # Resolve bucket & base prefix from the workspace (same as in
    # update_coral_ontologies.py)
    # ------------------------------------------------------------------
    workspace = get_my_workspace()                     # <-- defined elsewhere
    s3a_root = workspace.home_paths[0]                # e.g. "s3a://cdm‑lake/users‑general‑warehouse/jmc"
    bucket, base_prefix = _parse_s3a_uri(s3a_root)

    # TSV and typedef files live under <base_prefix>/data/data/
    data_prefix = f"{base_prefix.rstrip('/')}/data/data/"

    # ------------------------------------------------------------------
    # Load typedef.json (multiline JSON)
    # ------------------------------------------------------------------
    typedef_path = f"s3a://{bucket}/{data_prefix}typedef.json"
    typedef_df = spark.read.option("multiline", "true").json(typedef_path)

    typedef_row = typedef_df.first()
    if typedef_row is None:
        raise RuntimeError(f"Unable to load typedef.json from {typedef_path}")

    typedef_data = typedef_row.asDict(recursive=True)

    system_types = typedef_data.get("system_types", [])
    static_types = typedef_data.get("static_types", [])
    all_type_defs = system_types + static_types

    # ------------------------------------------------------------------
    # Build a mapping from CORAL type name → CDM table name
    # ------------------------------------------------------------------
    type_to_table: Dict[str, str] = {}
    for tdef in system_types:
        type_to_table[tdef["name"]] = f"sys_{tdef['name'].lower()}"
    for tdef in static_types:
        type_to_table[tdef["name"]] = f"sdt_{tdef['name'].lower()}"

    # ------------------------------------------------------------------
    # Initialise the target database
    # ------------------------------------------------------------------
    spark.sql(f"CREATE DATABASE IF NOT EXISTS {db_name}")
    spark.sql(f"USE {db_name}")

    all_typedef_rows: List[Dict[str, Any]] = []

    # ------------------------------------------------------------------
    # Process each CORAL type
    # ------------------------------------------------------------------
    for tdef in all_type_defs:
        coral_type_name = tdef.get("name")
        if not coral_type_name:
            continue

        table_name = type_to_table[coral_type_name]          # e.g. sys_process or sdt_genome
        schema, typedef_rows, term_mappings = generate_schema(tdef, table_name, type_to_table)
        all_typedef_rows.extend(typedef_rows)

        table_comment = f"CDM table for CORAL type `{coral_type_name}`"

        # ------------------------------------------------------------------
        # Load TSV data (if requested) – handle arrays and term columns
        # ------------------------------------------------------------------
        if load_tsv:
            tsv_path = f"s3a://{bucket}/{data_prefix}{coral_type_name}.tsv"
            try:
                # --------------------------------------------------------------
                # 1) Read the raw TSV **without** a schema so we keep the
                #    original column names exactly as they appear in the file.
                # --------------------------------------------------------------
                df = (
                    spark.read.format("csv")
                    .option("header", "true")
                    .option("sep", "\t")
                    .option("inferSchema", "false")      # keep everything as string
                    .load(tsv_path)
                )

                # --------------------------------------------------------------
                # 2) Rename NON‑TERM columns to their CDM names.
                #    TERM columns are left untouched for now – they will be split
                #    later.
                # --------------------------------------------------------------
                rename_map: Dict[str, str] = {}
                for f in tdef.get("fields", []):
                    if f.get("scalar_type") == "term":
                        continue                      # keep the original name for now
                    orig = f["name"]
                    new_name = field_to_column_name(f, table_name, type_to_table)
                    rename_map[orig] = new_name

                for orig, new_name in rename_map.items():
                    if orig in df.columns:
                        df = df.withColumnRenamed(orig, new_name)

                # --------------------------------------------------------------
                # 3) Convert any ARRAY columns (now identified by their CDM names)
                #    from the string representation to a real Spark ArrayType.
                # --------------------------------------------------------------
                array_cols = [
                    f.name for f in schema.fields
                    if isinstance(f.dataType, ArrayType)
                ]
                for col in array_cols:
                    if col in df.columns:
                        df = df.withColumn(col, _convert_array_column(col))

                # --------------------------------------------------------------
                # 4) Split TERM columns (original combined column → two cols)
                # --------------------------------------------------------------
                for m in term_mappings:
                    orig = m["orig_name"]
                    if orig in df.columns:
                        df = _split_term_column(df, orig, m["id_col"], m["name_col"])

                # --------------------------------------------------------------
                # 5) Add any missing columns (they will be NULL) so that the final
                #    select can succeed even if a TSV omitted optional fields.
                # --------------------------------------------------------------
                final_order = [f.name for f in schema.fields]
                missing = set(final_order) - set(df.columns)
                for col in missing:
                    # Find the expected datatype from the schema
                    dt = next((f.dataType for f in schema.fields if f.name == col), StringType())
                    df = df.withColumn(col, F.lit(None).cast(dt))

                # --------------------------------------------------------------
                # 6) Re‑order columns to exactly match the CDM schema.
                # --------------------------------------------------------------
                df = df.select(*final_order)

            except Exception as exc:  # pragma: no cover
                logging.warning(
                    f"Could not load TSV for {coral_type_name} from {tsv_path}: {exc}"
                )
                # Fallback: create an empty DataFrame with the final schema
                df = spark.createDataFrame([], schema)
        else:
            # No TSV loading – create an empty DataFrame with the final schema
            df = spark.createDataFrame([], schema)

        # ------------------------------------------------------------------
        # Rename generic columns to be table‑specific
        # ------------------------------------------------------------------
        # ``name`` → <table>_name
        if "name" in df.columns:
            df = df.withColumnRenamed("name", f"{table_name}_name")
        # ``description`` → <table>_description
        if "description" in df.columns:
            df = df.withColumnRenamed("description", f"{table_name}_description")

        # ------------------------------------------------------------------
        # Debug: show schema and a few rows before writing
        # ------------------------------------------------------------------
        logging.info(f"--- Schema for table `{db_name}.{table_name}` ---")
        df.printSchema()
        logging.info(f"--- Sample rows for `{db_name}.{table_name}` (up to 5) ---")
        df.show(5, truncate=False)

        # ------------------------------------------------------------------
        # Drop the table if it already exists (environment‑specific handling)
        # ------------------------------------------------------------------
        spark.sql(f"DROP TABLE IF EXISTS {db_name}.{table_name}")

        # ------------------------------------------------------------------
        # Write the Delta table (no .mode("overwrite") needed)
        # ------------------------------------------------------------------
        df.write.format(fmt).option("comment", table_comment).saveAsTable(
            f"{db_name}.{table_name}"
        )
        logging.info(f"Created table {db_name}.{table_name}")

    # ------------------------------------------------------------------
    # Auxiliary table: sys_typedef
    # ------------------------------------------------------------------
    sys_typedef_df = build_typedefs_dataframe(spark, all_typedef_rows)

    logging.info(f"--- Schema for auxiliary table `{db_name}.sys_typedef` ---")
    sys_typedef_df.printSchema()
    logging.info(f"--- Sample rows for `{db_name}.sys_typedef` (up to 5) ---")
    sys_typedef_df.show(5, truncate=False)

    spark.sql(f"DROP TABLE IF EXISTS {db_name}.sys_typedef")
    sys_typedef_df.write.format(fmt).option("comment", "CORAL type definitions").saveAsTable(f"{db_name}.sys_typedef")
    logging.info(f"Created auxiliary table {db_name}.sys_typedef")

    logging.info("✅ CDM generation complete.")


# ------------------------------------------------------------------
# Execute when run inside a notebook
# ------------------------------------------------------------------
spark = get_spark_session()               # <-- defined elsewhere
db_name = "jmc_coral"
spark.sql(f"USE {db_name}")
generate_cdm(spark, db_name)
