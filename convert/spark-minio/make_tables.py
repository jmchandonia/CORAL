"""generated_cdm_code.py

This file is auto‑generated by ``to_spark.py``.  It contains a
``generate_cdm(spark)`` function that creates the CDM tables in a Spark
session (Delta Lake by default).  Edit the configuration variables at
the top of the file before running it in a notebook.

NOTE: The script assumes that a SparkSession with the appropriate
MinIO/S3 configuration is already available.
"""

import json
import logging
import os
import re
from typing import Any, Dict, List, Tuple

from pyspark.sql import SparkSession, functions as F
from pyspark.sql.types import (
    StructType,
    StructField,
    StringType,
    IntegerType,
    DoubleType,
    BooleanType,
    ArrayType,
    MapType,
    DataType,
)

# ------------------------------------------------------------------
# Helper: parse an ``s3a://`` URI into bucket + prefix
# ------------------------------------------------------------------
def _parse_s3a_uri(s3a_uri: str) -> Tuple[str, str]:
    """
    Split an ``s3a://`` URI into its bucket component and the *remaining*
    path (prefix).  The returned prefix does **not** include a leading
    slash and always ends with a slash (unless it is empty).

    Example
    -------
    >>> _parse_s3a_uri("s3a://cdm‑lake/users-general-warehouse/jmc")
    ('cdm‑lake', 'users-general-warehouse/jmc/')
    """
    if not s3a_uri.startswith("s3a://"):
        raise ValueError(f"Unexpected S3A URI: {s3a_uri}")

    # Strip the scheme
    path = s3a_uri[len("s3a://") :]  # e.g. "cdm-lake/users-general-warehouse/jmc"

    # Split on the first slash – everything before it is the bucket name
    parts = path.split("/", 1)
    bucket = parts[0]

    # The rest (if any) is the prefix; normalise it to end with a slash
    if len(parts) == 2:
        prefix = parts[1].rstrip("/") + "/"
    else:
        prefix = ""

    return bucket, prefix


# ------------------------------------------------------------------
# Normalisation utilities – enforce lower‑case, underscore‑separated names
# ------------------------------------------------------------------
def _normalize_name(name: str) -> str:
    """
    Convert an arbitrary column name to ``snake_case``:
      * spaces and hyphens become underscores
      * any non‑alphanumeric character becomes an underscore
      * collapse consecutive underscores
      * lower‑case the result
    """
    name = name.strip()
    name = re.sub(r"[ \-]+", "_", name)          # spaces / hyphens → _
    name = re.sub(r"[^0-9a-zA-Z_]", "_", name)   # everything else → _
    name = re.sub(r"_+", "_", name)              # collapse multiple _
    return name.lower().strip("_")


# ------------------------------------------------------------------
# CDM naming helpers
# ------------------------------------------------------------------
def map_scalar_type(scalar_type: str) -> DataType:
    """Map a CORAL ``scalar_type`` string to a Spark DataType."""
    if scalar_type.startswith("[") and scalar_type.endswith("]"):
        inner = scalar_type[1:-1].strip()
        return ArrayType(map_scalar_type(inner), containsNull=False)

    st = scalar_type.lower()
    if st in ("text", "term"):
        return StringType()
    if st == "int":
        return IntegerType()
    if st in ("float", "double"):
        return DoubleType()
    if st == "bool":
        return BooleanType()
    return StringType()


def parse_fk(fk_str: str) -> Tuple[str, bool]:
    """Parse a foreign‑key descriptor (returns referenced type, is_array)."""
    is_array = False
    if fk_str.startswith("[") and fk_str.endswith("]"):
        is_array = True
        fk_str = fk_str[1:-1].strip()
    # ``fk_str`` may be just the type name (e.g. "Genome") or include a field (e.g. "Genome.id")
    return fk_str.split(".", 1)[0], is_array


def fk_column_name(ref_table: str, is_array: bool) -> str:
    """Return the foreign‑key column name for a referenced table."""
    base = ref_table.lower()
    return f"{base}_ids" if is_array else f"{base}_id"


def rename_fk_column(original_name: str, ref_type: str, is_array: bool,
                     type_to_table: Dict[str, str]) -> str:
    """
    Rename a foreign‑key column according to the CDM conventions and then
    normalise it to snake_case.

    * If the original column name contains the referenced type (singular or plural),
      replace that part with the CDM table name (e.g. ``sdt_gene``) and
      append ``_id`` or ``_ids`` before any remaining suffix.
      Example: ``genes_changed`` → ``sdt_gene_ids_changed``.
    * If it does not contain the referenced type, append ``_<cdm_table>_id``
      (or ``_ids``) to the original name.
      Example: ``derived_from`` → ``derived_from_sdt_genome_id``.
    """
    target_table = type_to_table.get(ref_type, ref_type.lower())  # e.g. sdt_gene
    suffix = "_ids" if is_array else "_id"

    lower_orig = original_name.lower()
    ref_lower = ref_type.lower()
    ref_plural = ref_lower + "s"

    idx = -1
    match_len = 0
    if ref_plural in lower_orig:
        idx = lower_orig.find(ref_plural)
        match_len = len(ref_plural)
    elif ref_lower in lower_orig:
        idx = lower_orig.find(ref_lower)
        match_len = len(ref_lower)

    if idx != -1:
        prefix = lower_orig[:idx]
        suffix_part = lower_orig[idx + match_len :]  # keep whatever follows the match
        new_name = f"{prefix}{target_table}{suffix}{suffix_part}"
    else:
        new_name = f"{lower_orig}_{target_table}{suffix}"

    return _normalize_name(new_name)


def field_to_column_name(
    field: Dict[str, Any],
    current_table: str,
    type_to_table: Dict[str, str],
) -> str:
    """
    Return the CDM column name for a field, applying the CDM naming rules
    and normalising the result to snake_case.

    * Primary‑key → ``<table>_id``
    * User‑primary‑key (UPK) → ``name`` (later renamed to ``<table>_name``)
    * Foreign‑key → renamed according to ``rename_fk_column`` above
    * Otherwise → the original field name
    """
    if field.get("PK", False):
        col = f"{current_table}_id"
    elif field.get("UPK", False):
        col = "name"
    else:
        fk = field.get("FK")
        if fk:
            ref_type, is_array = parse_fk(fk)
            original_name = field.get("name")
            col = rename_fk_column(original_name, ref_type, is_array, type_to_table)
        else:
            col = field.get("name")
    return _normalize_name(col)


def process_field(
    field: Dict[str, Any],
    type_name: str,
    current_table: str,
    type_to_table: Dict[str, str],
) -> Tuple[List[StructField], List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    Convert a CORAL field definition into one or more Spark ``StructField``
    objects and a list of metadata dictionaries.

    * For regular fields a single column is produced.
    * For ``scalar_type == "term"`` two columns are produced:
        - ``<field>_sys_oterm_id`` (FK to ``sys_oterm``)
        - ``<field>_sys_oterm_name`` (the term name)
    """
    # ------------------------------------------------------------------
    # Helper to build a generic StructField + typedef row
    # ------------------------------------------------------------------
    def build(col_name: str, spark_type: DataType, nullable: bool,
              comment: str, is_term_id: bool = False) -> Tuple[StructField, Dict[str, Any]]:
        metadata = {
            "orig_name": field.get("name"),
            "type_term": field.get("type_term"),
            "required": field.get("required", False),
            "pk": field.get("PK", False),
            "upk": field.get("UPK", False),
            "comment": comment,
        }
        if "constraint" in field:
            metadata["constraint"] = (
                json.dumps(field["constraint"])
                if isinstance(field["constraint"], (list, dict))
                else field["constraint"]
            )
        if "units_term" in field:
            metadata["units_term"] = field["units_term"]
        if field.get("FK"):
            metadata["fk"] = field["FK"]
            metadata["fk_table"], metadata["fk_is_array"] = parse_fk(field["FK"])

        struct = StructField(col_name, spark_type, nullable=nullable, metadata=metadata)

        typedef = {
            "type_name": type_name,
            "field_name": field.get("name"),
            "cdm_column_name": col_name,
            "scalar_type": field.get("scalar_type", "text"),
            "required": field.get("required", False),
            "pk": field.get("PK", False),
            "upk": field.get("UPK", False),
            "fk": field.get("FK") if not is_term_id else "sys_oterm.id",
            "constraint": field.get("constraint"),
            "comment": comment,
            "units_term": field.get("units_term"),
            "type_term": field.get("type_term"),
        }
        return struct, typedef

    # ------------------------------------------------------------------
    # Normal case handling
    # ------------------------------------------------------------------
    col_name = field_to_column_name(field, current_table, type_to_table)
    spark_type = map_scalar_type(field.get("scalar_type", "text"))
    nullable = not field.get("required", False)

    # ------------------------------------------------------------------
    # Special handling for term fields
    # ------------------------------------------------------------------
    if field.get("scalar_type") == "term":
        base = _normalize_name(field.get("name"))
        id_col = f"{base}_sys_oterm_id"
        name_col = f"{base}_sys_oterm_name"

        # ID column – foreign key to sys_oterm
        id_comment = f"Foreign key to `sys_oterm` (term id for field `{field.get('name')}`)"
        id_field, id_typedef = build(id_col, StringType(), nullable, id_comment, is_term_id=True)

        # Name column – term name
        name_comment = f"Term name for field `{field.get('name')}`"
        name_field, name_typedef = build(name_col, StringType(), nullable, name_comment)

        return [id_field, name_field], [id_typedef, name_typedef],_col,
                "name_col": name_col,
                "required": field.get("required", False),
            }
        ]

    # ------------------------------------------------------------------
    # Regular (non‑term) field
    # ------------------------------------------------------------------
    if field.get("PK", False):
        comment = f"Primary key for table `{current_table}`"
    elif field.get("FK"):
        ref_type, _ = parse_fk(field["FK"])
        comment = f"Foreign key to `{type_to_table.get(ref_type, ref_type.lower())}`"
    else:
        comment = field.get("comment") or f"Field `{field.get('name')}`"

    struct, typedef = build(col_name, spark_type, nullable, comment)
    return [struct], [typedef], []


def generate_schema(
    type_def: Dict[str, Any],
    table_name: str,
    type_to_table: Dict[str, str],
) -> Tuple[StructType, List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    Build a Spark ``StructType`` for a CORAL type and collect typedef
    metadata, using the CDM naming conventions.

    Returns:
        schema          – complete StructType for the table
        typedef_rows    – list of typedef metadata rows (one per output column)
        term_mappings   – list of dicts describing term fields that need
                          post‑processing (splitting of combined TSV column)
    """
    struct_fields: List[StructField] = []
    typedef_rows: List[Dict[str, Any]] = []
    term_mappings: List[Dict[str, Any]] = []

    for f in type_def.get("fields", []):
        fields, rows, term_info = process_field(f, type_def.get("name"), table_name, type_to_table)
        struct_fields.extend(fields)
        typedef_rows.extend(rows)
        term_mappings.extend(term_info)

    # Ensure primary‑key column exists (if not already created by a PK field)
    pk_name = f"{table_name}_id"
    if not any(sf.name == pk_name for sf in struct_fields):
        struct_fields.insert(
            0,
            StructField(
                pk_name,
                StringType(),
                nullable=False,
                metadata={"comment": f"Primary key for table `{table_name}`"},
            ),
        )

    # Add a ``name`` column for user‑primary‑key (UPK) if needed
    if any(r["upk"] for r in typedef_rows) and not any(sf.name == "name" for sf in struct_fields):
        struct_fields.append(
            StructField(
                "name",
                StringType(),
                nullable=False,
                metadata={"comment": "User‑defined primary key (UPK)"},
            )
        )

    return StructType(struct_fields), typedef_rows, term_mappings


def build_typedefs_dataframe(spark, typedef_rows: List[Dict[str, Any]]):
    """Create the ``sys_typedef`` DataFrame."""
    schema = StructType(
        [
            StructField("type_name", StringType(), nullable=False),
            StructField("field_name", StringType(), nullable=False),
            StructField("cdm_column_name", StringType(), nullable=False),
            StructField("scalar_type", StringType(), nullable=True),
            StructField("required", BooleanType(), nullable=True),
            StructField("pk", BooleanType(), nullable=True),
            StructField("upk", BooleanType(), nullable=True),
            StructField("fk", StringType(), nullable=True),
            StructField("constraint", StringType(), nullable=True),
            StructField("comment", StringType(), nullable=True),
            StructField("units_term", StringType(), nullable=True),
            StructField("type_term", StringType(), nullable=True),
        ]
    )
    rows = [
        (
            r["type_name"],
            r["field_name"],
            r["cdm_column_name"],
            r["scalar_type"],
            r["required"],
            r["pk"],
            r["upk"],
            r["fk"],
            json.dumps(r["constraint"])
            if isinstance(r["constraint"], (list, dict))
            else r["constraint"],
            r["comment"],
            r["units_term"],
            r["type_term"],
        )
        for r in typedef_rows
    ]
    return spark.createDataFrame(rows, schema)


# ------------------------------------------------------------------
# Helper: make a TSV‑compatible schema (arrays become strings) and
# remember which columns were originally arrays.
# Also replace term‑split columns with the original combined column.
# ------------------------------------------------------------------
def _prepare_tsv_schema(
    target_schema: StructType,
    term_mappings: List[Dict[str, Any]],
) -> Tuple[StructType, List[str], List[Dict[str, Any]]]:
    """
    Produce a schema suitable for reading the source TSV.

    * Array columns are converted to ``StringType`` (they will be split
      back to arrays after the file is read).
    * Columns that originate from a ``term`` field (``*_sys_oterm_id`` and
      ``*_sys_oterm_name``) are removed and replaced by a single column
      with the original field name (still ``StringType``).
    Returns:
        tsv_schema   – schema to use for ``spark.read``,
        array_cols   – list of column names that were originally arrays,
        term_info    – the unchanged ``term_mappings`` list (passed through).
    """
    term_cols = set()
    for m in term_mappings:
        term_cols.update([m["id_col"], m["name_col"]])

    new_fields = []
    array_cols = []
    for f in target_schema.fields:
        if f.name in term_cols:
            # skip the split term columns – they will be recreated later
            continue
        if isinstance(f.dataType, ArrayType):
            array_cols.append(f.name)
            new_fields.append(
                StructField(
                    f.name,
                    StringType(),
                    nullable=f.nullable,
                    metadata=f.metadata,
                )
            )
        else:
            new_fields.append(f)

    # add back the original combined term columns (as simple strings)
    for m in term_mappings:
        orig = m["orig_name"]
        # nullable follows the original required flag
        nullable = not m["required"]
        new_fields.append(
            StructField(
                orig,
                StringType(),
                nullable=nullable,
                metadata={"comment": f"Combined term field for `{orig}`"},
            )
        )

    return StructType(new_fields), array_cols, term_mappings


# ------------------------------------------------------------------
# Helper: convert the string representation of an array back to an
# actual Spark array of strings.
# ------------------------------------------------------------------
def _convert_array_column(col_name: str) -> Any:
    """
    Return a Column expression that turns the raw TSV string into an
    ArrayType(StringType).  Empty or null values become an empty array.
    """
    # 1️⃣ Strip surrounding brackets
    cleaned = F.regexp_replace(F.col(col_name), r'^\[|\]$', '')

    # 2️⃣ Split on commas (allow optional whitespace after the comma)
    splitted = F.split(cleaned, r',\s*')

    # 3️⃣ Remove empty strings that can appear when the original value was "[]"
    without_empty = F.array_remove(splitted, "")

    # 4️⃣ Preserve nulls / empty strings as empty arrays
    return F.when(F.col(col_name).isNull() | (F.col(col_name) == ''), F.array()).otherwise(without_empty)


# ------------------------------------------------------------------
# Helper: split a combined term column ``"<name> <id>"`` into two
# separate columns.
# ------------------------------------------------------------------
def _split_term_column(df, orig_name: str, id_col: str, name_col: str):
    """
    From a column that contains ``term name <term id>`` (or just one of the
    pieces) create two new columns: ``id_col`` and ``name_col``.
    The original column is dropped.
    """
    term_str = F.col(orig_name).cast("string")

    # Extract the ID inside angle brackets, if present
    id_extracted = F.when(
        term_str.isNull() | (term_str == ""),
        None,
    ).otherwise(F.regexp_extract(term_str, r"<([^>]+)>", 1))

    # The name is whatever precedes the opening angle bracket (trimmed)
    name_extracted = F.when(
        term_str.isNull() | (term_str == ""),
        None,
    ).otherwise(
        F.trim(
            F.regexp_replace(term_str, r"\s*<[^>]*>\s*$", "")
        )
    )

    df = df.withColumn(id_col, id_extracted).withColumn(name_col, name_extracted)
    df = df.drop(orig_name)
    return df


# ------------------------------------------------------------------
# Main entry point – to be called from a notebook
# ------------------------------------------------------------------
def generate_cdm(
    spark,
    db_name: str = "jmc_coral",
    load_tsv: bool = True,
    fmt: str = "delta",
):
    """
    Create the CDM tables in Spark.

    * System types → ``sys_<type>`` (e.g. ``sys_process``)
    * Static types → ``sdt_<type>`` (e.g. ``sdt_genome``)
    * Primary‑key column → ``<table>_id``
    * Foreign‑key column → ``<referenced‑table>_id`` (or ``_ids`` for arrays)
    * ``term`` fields are expanded to ``<field>_sys_oterm_id`` and
      ``<field>_sys_oterm_name``.

    The ``typedef.json`` and any TSV data files are read from the workspace
    S3‑A bucket under the prefix ``<base_prefix>/data/data/`` (the same
    layout used by ``update_coral_ontologies.py``).

    **Debugging** – before each table is written we log its schema and the
    first few rows (up to 5) so you can verify the parsing, especially of
    array and term columns.
    """
    # ------------------------------------------------------------------
    # Resolve bucket & base prefix from the workspace (same as in
    # update_coral_ontologies.py)
    # ------------------------------------------------------------------
    workspace = get_my_workspace()                     # <-- defined elsewhere
    s3a_root = workspace.home_paths[0]                # e.g. "s3a://cdm-lake/users-general-warehouse/jmc"
    bucket, base_prefix = _parse_s3a_uri(s3a_root)

    # The typedef and TSV files live under <base_prefix>/data/data/
    data_prefix = f"{base_prefix.rstrip('/')}/data/data/"

    # ------------------------------------------------------------------
    # Load typedef.json from S3A (multiline JSON)
    # ------------------------------------------------------------------
    typedef_path = f"s3a://{bucket}/{data_prefix}typedef.json"
    typedef_df = spark.read.option("multiline", "true").json(typedef_path)

    typedef_row = typedef_df.first()
    if typedef_row is None:
        raise RuntimeError(f"Unable to load typedef.json from {typedef_path}")

    typedef_data = typedef_row.asDict(recursive=True)

    system_types = typedef_data.get("system_types", [])
    static_types = typedef_data.get("static_types", [])
    all_type_defs = system_types + static_types

    # ------------------------------------------------------------------
    # Build a mapping from CORAL type name → CDM table name
    # ------------------------------------------------------------------
    type_to_table: Dict[str, str] = {}
    for tdef in system_types:
        type_to_table[tdef["name"]] = f"sys_{tdef['name'].lower()}"
    for tdef in static_types:
        type_to_table[tdef["name"]] = f"sdt_{tdef['name'].lower()}"

    # ------------------------------------------------------------------
    # Initialise the target database
    # ------------------------------------------------------------------
    spark.sql(f"CREATE DATABASE IF NOT EXISTS {db_name}")
    spark.sql(f"USE {db_name}")

    all_typedef_rows: List[Dict[str, Any]] = []

    # ------------------------------------------------------------------
    # Process each type definition
    # ------------------------------------------------------------------
    for tdef in all_type_defs:
        coral_type_name = tdef.get("name")
        if not coral_type_name:
            continue

        table_name = type_to_table[coral_type_name]          # e.g. sys_process or sdt_genome
        schema, typedef_rows, term_mappings = generate_schema(tdef, table_name, type_to_table)
        all_typedef_rows.extend(typedef_rows)

        table_comment = f"CDM table for CORAL type `{coral_type_name}`"

        # ------------------------------------------------------------------
        # Load TSV data from S3A (if requested) – handling arrays and term cols
        # ------------------------------------------------------------------
        if load_tsv:
            tsv_path = f"s3a://{bucket}/{data_prefix}{coral_type_name}.tsv"
            try:
                # Build a schema that can be used to read the raw TSV
                tsv_schema, array_columns, term_info = _prepare_tsv_schema(schema, term_mappings)

                df = (
                    spark.read.format("csv")
                    .option("header", "true")
                    .option("sep", "\t")
                    .schema(tsv_schema)
                    .load(tsv_path)
                )

                # Convert stringified arrays back to proper ArrayType columns
                for col in array_columns:
                    df = df.withColumn(col, _convert_array_column(col))

                # Split combined term columns into the two CDM columns
                for m in term_info:
                    orig = m["orig_name"]
                    if orig in df.columns:
                        df = _split_term_column(df, orig, m["id_col"], m["name_col"])

            except Exception as exc:  # pragma: no cover
                logging.warning(
                    f"Could not load TSV for {coral_type_name} from {tsv_path}: {exc}"
                )
                # Fallback: create an empty DataFrame with the final schema
                df = spark.createDataFrame([], schema)
        else:
            # No TSV loading – create an empty DataFrame with the final schema
            df = spark.createDataFrame([], schema)

        # ------------------------------------------------------------------
        # Rename generic columns to be table‑specific
        # ------------------------------------------------------------------
        # ``name`` → <table>_name
        if "name" in df.columns:
            df = df.withColumnRenamed("name", f"{table_name}_name")
        # ``description`` → <table>_description
        if "description" in df.columns:
            df = df.withColumnRenamed("description", f"{table_name}_description")

        # ------------------------------------------------------------------
        # Debug: show schema and a few rows before writing
        # ------------------------------------------------------------------
        logging.info(f"--- Schema for table `{db_name}.{table_name}` ---")
        df.printSchema()
        logging.info(f"--- Sample rows for `{db_name}.{table_name}` (up to 5) ---")
        df.show(5, truncate=False)

        # ------------------------------------------------------------------
        # Drop the table if it already exists (environment‑specific handling)
        # ------------------------------------------------------------------
        spark.sql(f"DROP TABLE IF EXISTS {db_name}.{table_name}")

        # ------------------------------------------------------------------
        # Write the Delta table (no .mode("overwrite") needed)
        # ------------------------------------------------------------------
        df.write.format(fmt).option("comment", table_comment).saveAsTable(f"{db_name}.{table_name}")
        logging.info(f"Created table {db_name}.{table_name}")

    # ------------------------------------------------------------------
    # Auxiliary table: sys_typedef
    # ------------------------------------------------------------------
    sys_typedef_df = build_typedefs_dataframe(spark, all_typedef_rows)

    logging.info(f"--- Schema for auxiliary table `{db_name}.sys_typedef` ---")
    sys_typedef_df.printSchema()
    logging.info(f"--- Sample rows for `{db_name}.sys_typedef` (up to 5) ---")
    sys_typedef_df.show(5, truncate=False)

    spark.sql(f"DROP TABLE IF EXISTS {db_name}.sys_typedef")
    sys_typedef_df.write.format(fmt) \
        .option("comment", "CORAL type definitions") \
        .saveAsTable(f"{db_name}.sys_typedef")
    logging.info(f"Created auxiliary table {db_name}.sys_typedef")

    logging.info("✅ CDM generation complete.")


# ------------------------------------------------------------------
# Execute the script when run inside a notebook
# ------------------------------------------------------------------
spark = get_spark_session()               # <-- defined elsewhere
db_name = "jmc_coral"
spark.sql(f"USE {db_name}")
generate_cdm(spark, db_name)
