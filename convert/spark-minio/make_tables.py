"""make_tables.py

This file is auto‑generated by ``to_spark.py``.  It contains a
``generate_cdm(spark)`` function that builds the CDM tables in a Spark
session (Delta Lake format).  The script follows the conventions used in
``update_coral_ontologies.py`` and adds robust handling for the
``scalar_type == "term"`` fields as well as a new *preferred_name* feature.

Key behaviours
--------------
* System types → tables named ``sys_<type>`` (e.g. ``sys_process``)
* Static types → tables named ``sdt_<type>`` (e.g. ``sdt_genome``)
* If a type defines ``preferred_name``, the CDM table uses that name
  (e.g. ``OTU`` → ``ASV`` → table ``sdt_asv``).
* Primary‑key column → ``<table>_id`` (values are rewritten to the
  preferred name when applicable).
* Foreign‑key column → ``<referenced_table>_name`` (or ``_names`` for arrays);
  column names and the stored values reference the name field.
* ``scalar_type == "term"`` fields are expanded **in place** into two
  columns: ``<field>_sys_oterm_id`` (FK → ``sys_oterm``) and
  ``<field>_sys_oterm_name`` (the term name)
* All column names are lower‑case snake_case (underscores only)
* Missing columns in a TSV are added as ``NULL`` so that the final schema
  can always be satisfied.
* Before each table is written we log its schema and the first few rows
  (up to 5) for debugging.
* Each table is dropped first (``DROP TABLE IF EXISTS``) to guarantee a
  clean creation.
* **Quoted strings that span multiple lines are now read correctly** – the
  TSV reader is configured with ``multiLine=True``.

Run the script in a notebook as follows::

    spark = get_spark_session()               # <-- defined elsewhere
    db_name = "enigma_coral"
    spark.sql(f"USE {db_name}")
    generate_cdm(spark, db_name)
"""

import json
import logging
import os
import re
from typing import Any, Dict, List, Tuple

from pyspark.sql import SparkSession, functions as F
from pyspark.sql.types import (
    StructType,
    StructField,
    StringType,
    IntegerType,
    DoubleType,
    BooleanType,
    ArrayType,
    DataType,
)

# ------------------------------------------------------------------
# Constants: URL rewrites in raw TSV content
# ------------------------------------------------------------------
_URL_REWRITE_FROM = "https://genomics.lbl.gov/enigma-data/"
_URL_REWRITE_TO = "enigma-data-repository/"

# ------------------------------------------------------------------
# Helper: parse an ``s3a://`` URI into bucket + prefix
# ------------------------------------------------------------------
def _parse_s3a_uri(s3a_uri: str) -> Tuple[str, str]:
    """Return (bucket, prefix) from a ``s3a://`` URI."""
    if not s3a_uri.startswith("s3a://"):
        raise ValueError(f"Unexpected S3A URI: {s3a_uri}")

    path = s3a_uri[len("s3a://") :]          # e.g. "cdm‑lake/users‑general‑warehouse/jmc"
    parts = path.split("/", 1)
    bucket = parts[0]
    prefix = parts[1].rstrip("/") + "/" if len(parts) == 2 else ""
    return bucket, prefix


# ------------------------------------------------------------------
# Normalise column names (lower‑case, underscores only)
# ------------------------------------------------------------------
def _normalize_name(name: str) -> str:
    """Convert an arbitrary name to snake_case."""
    name = name.strip()
    name = re.sub(r"[ \-]+", "_", name)            # spaces / hyphens → _
    name = re.sub(r"[^0-9a-zA-Z_]", "_", name)     # everything else → _
    name = re.sub(r"_+", "_", name)                # collapse multiple _
    return name.lower().strip("_")


# ------------------------------------------------------------------
# CDM naming helpers
# ------------------------------------------------------------------
def map_scalar_type(scalar_type: str) -> DataType:
    """Map a CORAL ``scalar_type`` to a Spark DataType."""
    if scalar_type.startswith("[") and scalar_type.endswith("]"):
        inner = scalar_type[1:-1].strip()
        return ArrayType(map_scalar_type(inner), containsNull=False)

    st = scalar_type.lower()
    if st in ("text", "term"):
        return StringType()
    if st == "int":
        return IntegerType()
    if st in ("float", "double"):
        return DoubleType()
    if st == "bool":
        return BooleanType()
    return StringType()


def parse_fk(fk_str: str) -> Tuple[str, bool]:
    """Parse a foreign‑key descriptor, returning (type, is_array)."""
    is_array = False
    if fk_str.startswith("[") and fk_str.endswith("]"):
        is_array = True
        fk_str = fk_str[1:-1].strip()
    return fk_str.split(".", 1)[0], is_array


def rename_fk_column(original_name: str, ref_type: str, is_array: bool,
                    type_to_table: Dict[str, str]) -> str:
    """
    Rename a foreign‑key column according to CDM conventions and then
    normalise it to snake_case.

    * If the original column name contains the referenced type (singular or
      plural), replace that part with the CDM table name and insert ``_name``
      or ``_names`` **before** any remaining suffix.
      Example: ``genes_changed`` → ``sdt_gene_names_changed``.
    * If it does not contain the referenced type, append ``_<cdm_table>_name``
      (or ``_names``) to the original name.
      Example: ``derived_from`` → ``derived_from_sdt_genome_name``.
    """
    target_table = type_to_table.get(ref_type, ref_type.lower())
    suffix = "_names" if is_array else "_name"

    lower_orig = original_name.lower()
    ref_singular = ref_type.lower()
    ref_plural = ref_singular + "s"

    idx = -1
    match_len = 0
    if ref_plural in lower_orig:
        idx = lower_orig.find(ref_plural)
        match_len = len(ref_plural)
    elif ref_singular in lower_orig:
        idx = lower_orig.find(ref_singular)
        match_len = len(ref_singular)

    if idx != -1:
        prefix = lower_orig[:idx]
        suffix_part = lower_orig[idx + match_len :]   # keep whatever follows the match
        new_name = f"{prefix}{target_table}{suffix}{suffix_part}"
    else:
        new_name = f"{lower_orig}_{target_table}{suffix}"

    return _normalize_name(new_name)


def field_to_column_name(field: Dict[str, Any],
                         current_table: str,
                         type_to_table: Dict[str, str],
                         units_lookup: Dict[str, str] = None) -> str:
    """
    Derive the CDM column name for a field (non‑term fields only) and
    normalise it.
    """
    if field.get("PK", False):
        return _normalize_name(f"{current_table}_id")
    if field.get("UPK", False):
        return _normalize_name(f"{current_table}_name")
    fk = field.get("FK")
    if fk:
        ref_type, is_array = parse_fk(fk)
        return rename_fk_column(field["name"], ref_type, is_array, type_to_table)
    
    # Handle description field
    if _normalize_name(field["name"]) == "description":
        return _normalize_name(f"{current_table}_description")
    
    base_name = _normalize_name(field["name"])
    
    # For numeric fields with units, append the unit name to the column name
    scalar_type = field.get("scalar_type", "text").lower()
    if scalar_type in ("int", "float", "double") and field.get("units_term") and units_lookup:
        unit_name = units_lookup.get(field["units_term"])
        if unit_name:
            unit_normalized = _normalize_name(unit_name)
            base_name = f"{base_name}_{unit_normalized}"
    
    return base_name


# ------------------------------------------------------------------
# Helper: replace ID prefixes inside a column (string or array)
# ------------------------------------------------------------------
def _replace_id_prefix(df, col_name: str, old_prefix: str, new_prefix: str,
                       is_array: bool) -> Any:
    """
    For a column that stores IDs, replace the leading ``old_prefix`` with
    ``new_prefix``.  Works for plain strings and for arrays of strings.
    """
    if is_array:
        # Transform each element of the array
        df = df.withColumn(
            col_name,
            F.transform(
                F.col(col_name),
                lambda x: F.when(
                    x.isNull(),
                    None
                ).otherwise(
                    F.regexp_replace(x, f"^{re.escape(old_prefix)}", new_prefix)
                ),
            ),
        )
    else:
        df = df.withColumn(
            col_name,
            F.when(
                F.col(col_name).isNull(),
                None,
            ).otherwise(
                F.regexp_replace(F.col(col_name), f"^{re.escape(old_prefix)}", new_prefix)
            ),
        )
    return df


# ------------------------------------------------------------------
# Helper: replace a substring in all string columns
# ------------------------------------------------------------------
def _replace_text_in_string_cols(df, old_text: str, new_text: str):
    """Apply a literal substring replacement across all string columns."""
    if not old_text:
        return df
    old_regex = re.escape(old_text)
    for field in df.schema.fields:
        if isinstance(field.dataType, StringType):
            df = df.withColumn(
                field.name,
                F.regexp_replace(F.col(field.name), old_regex, new_text),
            )
    return df


# ------------------------------------------------------------------
# Helper: build JSON comment with metadata
# ------------------------------------------------------------------
def _build_json_comment(field: Dict[str, Any], 
                        description: str,
                        fk: str = None,
                        type_to_table: Dict[str, str] = None,
                        is_pk: bool = False,
                        is_upk: bool = False,
                        unit_name: str = None) -> str:
    """
    Build a JSON comment string with field metadata.
    """
    comment_dict = {"description": description}
    
    # Add units if present (using unit name instead of ID)
    if unit_name:
        comment_dict["unit"] = unit_name
    
    # Add type information
    if is_pk:
        comment_dict["type"] = "primary_key"
    elif is_upk:
        comment_dict["type"] = "unique_key"
    elif fk:
        comment_dict["type"] = "foreign_key"
        # Parse the FK to get the table and column reference
        ref_type, _ = parse_fk(fk)
        
        # Special handling for sys_oterm references
        if ref_type == "sys_oterm" or fk.startswith("sys_oterm."):
            comment_dict["references"] = "sys_oterm.sys_oterm_id"
        else:
            ref_table = type_to_table.get(ref_type, ref_type.lower()) if type_to_table else ref_type.lower()
            # Foreign keys to other tables reference the _name column
            comment_dict["references"] = f"{ref_table}.{ref_table}_name"
    
    return json.dumps(comment_dict)


# ------------------------------------------------------------------
# Process a single field – may produce one or two CDM columns
# ------------------------------------------------------------------
def process_field(field: Dict[str, Any],
                  type_name: str,
                  current_table: str,
                  type_to_table: Dict[str, str],
                  units_lookup: Dict[str, str] = None) -> Tuple[
                      List[StructField],
                      List[Dict[str, Any]],
                      List[Dict[str, Any]]
                  ]:
    """
    Convert a CORAL field definition into Spark ``StructField`` objects,
    rows for the ``sys_typedef`` table, and (if needed) term‑mapping
    dictionaries used later to split the combined TSV column.
    """
    # Resolve unit name if present
    unit_name = None
    if field.get("units_term") and units_lookup:
        unit_name = units_lookup.get(field["units_term"])
    
    def build(col_name: str,
              spark_type: DataType,
              nullable: bool,
              plain_comment: str,
              fk: str = None,
              is_pk: bool = False,
              is_upk: bool = False) -> Tuple[StructField, Dict[str, Any]]:
        
        # Build JSON comment for the table metadata
        json_comment = _build_json_comment(field, plain_comment, fk, type_to_table, is_pk, is_upk, unit_name)
        
        metadata = {
            "orig_name": field.get("name"),
            "type_sys_oterm_id": field.get("type_term"),
            "is_required": field.get("required", False),
            "is_pk": field.get("PK", False),
            "is_upk": field.get("UPK", False),
            "comment": json_comment,
        }
        if "constraint" in field:
            metadata["constraint"] = (
                json.dumps(field["constraint"])
                if isinstance(field["constraint"], (list, dict))
                else field["constraint"]
            )
        if "units_term" in field:
            metadata["units_sys_oterm_id"] = field["units_term"]
        if fk:
            metadata["fk"] = fk

        struct = StructField(col_name, spark_type, nullable=nullable, metadata=metadata)

        # cdm_column_name is already table-specific from field_to_column_name
        typedef = {
            "type_name": type_name,
            "field_name": field.get("name"),
            "cdm_column_name": col_name,
            "scalar_type": field.get("scalar_type", "text"),
            "is_required": field.get("required", False),
            "is_pk": field.get("PK", False),
            "is_upk": field.get("UPK", False),
            "fk": fk,
            "constraint": field.get("constraint"),
            "comment": plain_comment,
            "units_sys_oterm_id": field.get("units_term"),
            "type_sys_oterm_id": field.get("type_term"),
        }
        return struct, typedef

    # ------------------------------------------------------------------
    # TERM fields – split into id + name columns (keep original order)
    # ------------------------------------------------------------------
    if field.get("scalar_type") == "term":
        base = _normalize_name(field.get("name"))
        id_col = f"{base}_sys_oterm_id"
        name_col = f"{base}_sys_oterm_name"

        # Use the comment from typedef.json if present, otherwise use autogenerated comments
        user_comment = field.get("comment")
        
        if user_comment:
            # Use the user's comment for id column, and append ", ontology term CURIE"
            id_comment = f"{user_comment}, ontology term CURIE"
            # Use the user's comment as-is for name column
            name_comment = user_comment
        else:
            # Use autogenerated comments
            id_comment = f"Foreign key to `sys_oterm` (term id for field `{field.get('name')}`), ontology term CURIE"
            name_comment = f"Term name for field `{field.get('name')}`"

        id_field, id_typedef = build(
            id_col,
            StringType(),
            not field.get("required", False),
            id_comment,
            fk="sys_oterm.id",
        )
        name_field, name_typedef = build(
            name_col,
            StringType(),
            not field.get("required", False),
            name_comment,
        )

        term_mapping = {
            "orig_name": field.get("name"),   # column name as it appears in the TSV
            "id_col": id_col,
            "name_col": name_col,
            "required": field.get("required", False),
        }
        return [id_field, name_field], [id_typedef, name_typedef], [term_mapping]

    # ------------------------------------------------------------------
    # NON‑TERM fields – regular handling
    # ------------------------------------------------------------------
    col_name = field_to_column_name(field, current_table, type_to_table, units_lookup)
    spark_type = map_scalar_type(field.get("scalar_type", "text"))
    nullable = not field.get("required", False)

    # Determine the plain comment (from typedef.json or autogenerated)
    if field.get("PK", False):
        plain_comment = field.get("comment") or f"Primary key for table `{current_table}`"
        struct, typedef = build(col_name, spark_type, nullable, plain_comment,
                               is_pk=True)
    elif field.get("UPK", False):
        plain_comment = field.get("comment") or f"User-defined unique key for table `{current_table}`"
        struct, typedef = build(col_name, spark_type, nullable, plain_comment,
                               is_upk=True)
    elif field.get("FK"):
        ref_type, _ = parse_fk(field["FK"])
        plain_comment = field.get("comment") or f"Foreign key to `{type_to_table.get(ref_type, ref_type.lower())}`"
        struct, typedef = build(col_name, spark_type, nullable, plain_comment,
                               fk=field.get("FK"))
    else:
        plain_comment = field.get("comment") or f"Field `{field.get('name')}`"
        struct, typedef = build(col_name, spark_type, nullable, plain_comment)
    
    return [struct], [typedef], []


# ------------------------------------------------------------------
# Build the full schema for a type, collecting term‑split info
# ------------------------------------------------------------------
def generate_schema(type_def: Dict[str, Any],
                    table_name: str,
                    type_to_table: Dict[str, str],
                    units_lookup: Dict[str, str] = None) -> Tuple[
                        StructType,
                        List[Dict[str, Any]],
                        List[Dict[str, Any]]
                    ]:
    """
    Return (schema, typedef_rows, term_mappings) for a CORAL type.
    The schema preserves the order of fields, expanding term fields into the
    two generated columns in the same position as the original field.
    """
    struct_fields: List[StructField] = []
    typedef_rows: List[Dict[str, Any]] = []
    term_mappings: List[Dict[str, Any]] = []

    for f in type_def.get("fields", []):
        fields, rows, terms = process_field(f, type_def.get("name"), table_name, type_to_table, units_lookup)
        struct_fields.extend(fields)
        typedef_rows.extend(rows)
        term_mappings.extend(terms)

    # Ensure a primary‑key column exists (if not already added by a PK field)
    pk_name = f"{table_name}_id"
    if not any(sf.name == pk_name for sf in struct_fields):
        plain_comment = f"Primary key for table `{table_name}`"
        json_comment = json.dumps({"description": plain_comment, "type": "primary_key"})
        struct_fields.insert(
            0,
            StructField(
                pk_name,
                StringType(),
                nullable=False,
                metadata={"comment": json_comment},
            ),
        )
        # Add corresponding typedef row
        typedef_rows.insert(
            0,
            {
                "type_name": type_def.get("name"),
                "field_name": "id",
                "cdm_column_name": pk_name,
                "scalar_type": "text",
                "is_required": True,
                "is_pk": True,
                "is_upk": False,
                "fk": None,
                "constraint": None,
                "comment": plain_comment,
                "units_sys_oterm_id": None,
                "type_sys_oterm_id": None,
            }
        )

    return StructType(struct_fields), typedef_rows, term_mappings


# ------------------------------------------------------------------
# Build the auxiliary ``sys_typedef`` DataFrame
# ------------------------------------------------------------------
def build_typedefs_dataframe(spark, typedef_rows: List[Dict[str, Any]], sys_oterm_df):
    schema = StructType([
        StructField("type_name", StringType(), nullable=False),
        StructField("field_name", StringType(), nullable=False),
        StructField("cdm_column_name", StringType(), nullable=False),
        StructField("scalar_type", StringType(), nullable=True),
        StructField("is_required", BooleanType(), nullable=False),
        StructField("is_pk", BooleanType(), nullable=False),
        StructField("is_upk", BooleanType(), nullable=False),
        StructField("fk", StringType(), nullable=True),
        StructField("constraint", StringType(), nullable=True),
        StructField("comment", StringType(), nullable=True),
        StructField("units_sys_oterm_id", StringType(), nullable=True),
        StructField("units_sys_oterm_name", StringType(), nullable=True),
        StructField("type_sys_oterm_id", StringType(), nullable=True),
        StructField("type_sys_oterm_name", StringType(), nullable=True),
    ])

    rows = [
        (
            r["type_name"],
            r["field_name"],
            r["cdm_column_name"],
            r["scalar_type"],
            r["is_required"] if r["is_required"] is not None else False,
            r["is_pk"] if r["is_pk"] is not None else False,
            r["is_upk"] if r["is_upk"] is not None else False,
            r["fk"],
            json.dumps(r["constraint"])
            if isinstance(r["constraint"], (list, dict))
            else r["constraint"],
            r["comment"],
            r["units_sys_oterm_id"],
            None,  # units_sys_oterm_name - will be populated via join
            r["type_sys_oterm_id"],
            None,  # type_sys_oterm_name - will be populated via join
        )
        for r in typedef_rows
    ]
    df = spark.createDataFrame(rows, schema)
    
    # Join with sys_oterm to populate units_sys_oterm_name
    df = df.alias("t").join(
        sys_oterm_df.alias("u"),
        F.col("t.units_sys_oterm_id") == F.col("u.sys_oterm_id"),
        "left"
    ).select(
        "t.*",
        F.col("u.sys_oterm_name").alias("units_name_temp")
    )
    
    df = df.withColumn(
        "units_sys_oterm_name",
        F.col("units_name_temp")
    ).drop("units_name_temp")
    
    # Join with sys_oterm to populate type_sys_oterm_name
    df = df.alias("t").join(
        sys_oterm_df.alias("ty"),
        F.col("t.type_sys_oterm_id") == F.col("ty.sys_oterm_id"),
        "left"
    ).select(
        "t.*",
        F.col("ty.sys_oterm_name").alias("type_name_temp")
    )
    
    df = df.withColumn(
        "type_sys_oterm_name",
        F.col("type_name_temp")
    ).drop("type_name_temp")
    
    # Reorder columns to match schema
    df = df.select(
        "type_name",
        "field_name",
        "cdm_column_name",
        "scalar_type",
        "is_required",
        "is_pk",
        "is_upk",
        "fk",
        "constraint",
        "comment",
        "units_sys_oterm_id",
        "units_sys_oterm_name",
        "type_sys_oterm_id",
        "type_sys_oterm_name"
    )
    
    return df


# ------------------------------------------------------------------
# Helper: convert a stringified array back to an actual ArrayType column
# ------------------------------------------------------------------
def _convert_array_column(col_name: str) -> Any:
    """
    Turn a TSV string like ``[a,b,c]`` into an ``ArrayType(String)``.
    Empty or null values become an empty array.
    """
    cleaned = F.regexp_replace(F.col(col_name), r"^\[|\]$", "")
    splitted = F.split(cleaned, r",\s*")
    without_empty = F.array_remove(splitted, "")
    return F.when(F.col(col_name).isNull() | (F.col(col_name) == ""), F.array()).otherwise(without_empty)


# ------------------------------------------------------------------
# Helper: split a combined term column ``"<name> <id>"`` into two columns
# ------------------------------------------------------------------
def _split_term_column(df,
                       orig_name: str,
                       id_col: str,
                       name_col: str):
    """
    From a column that contains ``term name <term id>`` (or only one part)
    create two new columns: ``id_col`` and ``name_col`` and drop the
    original column.
    """
    term_str = F.col(orig_name).cast("string")

    # ---- extract the ID (text inside <...>) ---------------------------------
    raw_id = F.regexp_extract(term_str, r"<([^>]+)>", 1)          # empty string if no match
    id_extracted = F.when(raw_id == "", None).otherwise(raw_id)

    # ---- extract the name (everything before the angle‑bracket part) -------
    raw_name = F.regexp_extract(term_str, r"^(.*?)(?:\s*<[^>]+>)?\s*$", 1)
    name_extracted = F.when(raw_name == "", None).otherwise(F.trim(raw_name))

    # ---- handle completely empty cells --------------------------------------
    id_extracted = F.when(term_str.isNull() | (term_str == ""), None).otherwise(id_extracted)
    name_extracted = F.when(term_str.isNull() | (term_str == ""), None).otherwise(name_extracted)

    df = df.withColumn(id_col, id_extracted).withColumn(name_col, name_extracted)
    df = df.drop(orig_name)
    return df


# ------------------------------------------------------------------
# Helper: Create a nullable version of a schema
# ------------------------------------------------------------------
def _make_schema_nullable(schema: StructType) -> StructType:
    """
    Create a copy of the schema with all fields marked as nullable.
    This is used when loading TSV data to handle missing or incomplete data.
    """
    nullable_fields = []
    for field in schema.fields:
        nullable_fields.append(
            StructField(field.name, field.dataType, nullable=True, metadata=field.metadata)
        )
    return StructType(nullable_fields)


# ------------------------------------------------------------------
# Helper: Cast DataFrame columns to match schema types
# ------------------------------------------------------------------
def _cast_columns_to_schema(df, schema: StructType):
    """
    Cast DataFrame columns to match the types defined in the schema.
    This is needed because TSV files are read with all string columns.
    """
    for field in schema.fields:
        if field.name in df.columns:
            df = df.withColumn(field.name, F.col(field.name).cast(field.dataType))
    return df


# ------------------------------------------------------------------
# Helper: Apply schema with metadata to a DataFrame
# ------------------------------------------------------------------
def _apply_schema_with_metadata(spark, df, schema: StructType, make_nullable: bool = False):
    """
    Apply a schema with metadata (including comments) to a DataFrame.
    This ensures that column comments are preserved when writing to Delta tables.
    """
    target_schema = _make_schema_nullable(schema) if make_nullable else schema
    
    # Check if DataFrame is empty using count instead of rdd
    if df.limit(1).count() == 0:
        return spark.createDataFrame([], target_schema)
    
    # Cast columns to match schema types
    df = _cast_columns_to_schema(df, target_schema)
    
    # Build a projection that reapplies types and field metadata in pure Spark.
    # This avoids pandas/Arrow conversion issues on large or irregular TSVs.
    projected_cols = []
    for field in target_schema.fields:
        col_expr = F.col(field.name).cast(field.dataType)
        if field.metadata:
            try:
                col_expr = col_expr.alias(field.name, metadata=field.metadata)
            except TypeError:
                # Older Spark/PySpark versions may not support alias(metadata=...).
                col_expr = col_expr.alias(field.name)
        else:
            col_expr = col_expr.alias(field.name)
        projected_cols.append(col_expr)

    return df.select(*projected_cols)


# ------------------------------------------------------------------
# Main entry point – to be called from a notebook
# ------------------------------------------------------------------
def generate_cdm(spark,
                 db_name: str = "enigma_coral",
                 load_tsv: bool = True,
                 fmt: str = "delta"):
    """
    Build the CDM tables.

    * System types → ``sys_<type>`` (e.g. ``sys_process``)
    * Static types → ``sdt_<type>`` (e.g. ``sdt_genome``)
    * ``term`` fields become ``<field>_sys_oterm_id`` and
      ``<field>_sys_oterm_name`` in the same position as the original column.
    * If a type defines a ``preferred_name`` the table and all its IDs are
      renamed accordingly.
    * All column names are lower‑case snake_case.
    """
    # ------------------------------------------------------------------
    # Resolve bucket & base prefix from the workspace (same as in
    # update_coral_ontologies.py)
    # ------------------------------------------------------------------
    workspace = get_my_workspace()                     # <-- defined elsewhere
    s3a_root = workspace.home_paths[0]                # e.g. "s3a://cdm‑lake/users‑general‑warehouse/jmc"
    bucket, base_prefix = _parse_s3a_uri(s3a_root)

    # TSV and typedef files live under <base_prefix>/data/coral/data/
    data_prefix = f"{base_prefix.rstrip('/')}/data/coral/data/"

    # ------------------------------------------------------------------
    # Load sys_oterm table for unit lookups
    # ------------------------------------------------------------------
    try:
        sys_oterm_df = spark.table(f"{db_name}.sys_oterm")
        # Create lookup dictionary from sys_oterm_id to sys_oterm_name
        units_lookup = {row["sys_oterm_id"]: row["sys_oterm_name"] 
                       for row in sys_oterm_df.select("sys_oterm_id", "sys_oterm_name").collect()}
    except Exception as e:
        logging.warning(f"Could not load sys_oterm table for unit lookups: {e}")
        sys_oterm_df = None
        units_lookup = {}

    # ------------------------------------------------------------------
    # Load typedef.json (multiline JSON)
    # ------------------------------------------------------------------
    typedef_path = f"s3a://{bucket}/{data_prefix}typedef.json"
    typedef_df = spark.read.option("multiline", "true").json(typedef_path)

    typedef_row = typedef_df.first()
    if typedef_row is None:
        raise RuntimeError(f"Unable to load typedef.json from {typedef_path}")

    typedef_data = typedef_row.asDict(recursive=True)

    system_types = typedef_data.get("system_types", [])
    static_types = typedef_data.get("static_types", [])
    all_type_defs = system_types + static_types

    # ------------------------------------------------------------------
    # Build a mapping from CORAL type name → preferred name (or itself)
    # ------------------------------------------------------------------
    preferred_name_map: Dict[str, str] = {}
    for tdef in system_types + static_types:
        original = tdef["name"]
        preferred = tdef.get("preferred_name")
        preferred_name_map[original] = preferred if preferred else original

    # ------------------------------------------------------------------
    # Build a mapping from CORAL type name → CDM table name, using the
    # preferred names where they exist.
    # ------------------------------------------------------------------
    type_to_table: Dict[str, str] = {}
    for tdef in system_types:
        orig = tdef["name"]
        pref = preferred_name_map[orig]
        type_to_table[orig] = f"sys_{pref.lower()}"
    for tdef in static_types:
        orig = tdef["name"]
        pref = preferred_name_map[orig]
        type_to_table[orig] = f"sdt_{pref.lower()}"

    # ------------------------------------------------------------------
    # Initialise the target database
    # ------------------------------------------------------------------
    spark.sql(f"CREATE DATABASE IF NOT EXISTS {db_name}")
    spark.sql(f"USE {db_name}")

    all_typedef_rows: List[Dict[str, Any]] = []

    # ------------------------------------------------------------------
    # Process each CORAL type
    # ------------------------------------------------------------------
    for tdef in all_type_defs:
        coral_type_name = tdef.get("name")
        if not coral_type_name:
            continue

        # -------------------- table naming --------------------
        table_name = type_to_table[coral_type_name]          # already incorporates preferred name
        schema, typedef_rows, term_mappings = generate_schema(tdef, table_name, type_to_table, units_lookup)
        all_typedef_rows.extend(typedef_rows)

        type_comment = (tdef.get("comment") or "").strip()
        table_comment = (
            type_comment
            if type_comment
            else f"CDM table for CORAL type `{coral_type_name}`"
        )

        # -------------------- load TSV (if requested) --------------------
        if load_tsv:
            tsv_path = f"s3a://{bucket}/{data_prefix}{coral_type_name}.tsv"
            try:
                # --------------------------------------------------------------
                # 1) Read raw TSV **with** multi‑line support so quoted strings
                #    that contain new‑lines are kept intact.
                # --------------------------------------------------------------
                df = (
                    spark.read.format("csv")
                    .option("header", "true")
                    .option("sep", "\t")
                    .option("inferSchema", "false")      # keep everything as string
                    .option("multiLine", "true")         # <-- important fix
                    .option("quote", "\"")               # default, kept for clarity
                    .option("escape", "\"")              # default escape character
                    .load(tsv_path)
                )

                # Replace legacy ENIGMA URLs before any column handling.
                df = _replace_text_in_string_cols(
                    df,
                    _URL_REWRITE_FROM,
                    _URL_REWRITE_TO,
                )

                # --------------------------------------------------------------
                # 2) Build rename map for NON‑TERM columns.
                #    We need to handle:
                #    a) Original field names (e.g., "location")
                #    b) Old CDM names (e.g., "location_id" for FK fields)
                #    c) Generic names like "name" and "description"
                # --------------------------------------------------------------
                rename_map: Dict[str, str] = {}
                
                # Add mapping for generic "name" column to table-specific name
                name_col = f"{table_name}_name"
                if "name" in df.columns and name_col in [f.name for f in schema.fields]:
                    rename_map["name"] = name_col
                
                # Add mapping for generic "description" column to table-specific description
                desc_col = f"{table_name}_description"
                if "description" in df.columns and desc_col in [f.name for f in schema.fields]:
                    rename_map["description"] = desc_col
                
                for f in tdef.get("fields", []):
                    if f.get("scalar_type") == "term":
                        continue                      # keep term column for now
                    
                    orig = f["name"]
                    new_name = field_to_column_name(f, table_name, type_to_table, units_lookup)
                    
                    # Skip if this is "name" or "description" - already handled above
                    if orig in ["name", "description"]:
                        continue
                    
                    # Map original field name to new CDM name
                    rename_map[orig] = new_name
                    
                    # For FK fields, also map the old CDM convention (e.g., "location_id")
                    # to the new convention (e.g., "sdt_location_name")
                    fk_str = f.get("FK")
                    if fk_str:
                        ref_type, is_array = parse_fk(fk_str)
                        # Old CDM convention used "<table>_id" or "<table>_ids"
                        old_suffix = "_ids" if is_array else "_id"
                        old_cdm_name = f"{ref_type.lower()}{old_suffix}"
                        
                        # Only add this mapping if it's different from the original field name
                        if old_cdm_name != orig:
                            rename_map[old_cdm_name] = new_name

                # Apply renames
                for old_col, new_col in rename_map.items():
                    if old_col in df.columns:
                        df = df.withColumnRenamed(old_col, new_col)
                        logging.debug(f"Renamed column '{old_col}' -> '{new_col}'")

                # --------------------------------------------------------------
                # 3) Convert ARRAY columns.
                # --------------------------------------------------------------
                array_cols = [
                    f.name for f in schema.fields
                    if isinstance(f.dataType, ArrayType)
                ]
                for col in array_cols:
                    if col in df.columns:
                        df = df.withColumn(col, _convert_array_column(col))

                # --------------------------------------------------------------
                # 4) Split TERM columns.
                # --------------------------------------------------------------
                for m in term_mappings:
                    orig = m["orig_name"]
                    if orig in df.columns:
                        df = _split_term_column(df, orig, m["id_col"], m["name_col"])

                # --------------------------------------------------------------
                # 5) Rewrite ID values when a preferred name is defined.
                # --------------------------------------------------------------
                # Primary‑key values (if the table itself has a preferred name)
                original_type = coral_type_name                # e.g. "OTU"
                preferred_type = preferred_name_map[original_type]
                if preferred_type != original_type:
                    pk_col = f"{table_name}_id"
                    if pk_col in df.columns:
                        df = _replace_id_prefix(df, pk_col, original_type, preferred_type,
                                                is_array=False)

                # Foreign‑key values – use metadata attached to the schema fields.
                for f in schema.fields:
                    meta = f.metadata
                    fk_desc = meta.get("fk")
                    if not fk_desc:
                        continue
                    # Strip possible ".id" suffix (used for term‑id columns)
                    ref_part = fk_desc.split(".")[0]
                    ref_type, _ = parse_fk(ref_part)
                    pref_ref = preferred_name_map.get(ref_type, ref_type)
                    if pref_ref != ref_type:
                        df = _replace_id_prefix(df, f.name, ref_type, pref_ref,
                                                is_array=isinstance(f.dataType, ArrayType))

                # --------------------------------------------------------------
                # 6) Add any missing columns (they will be NULL) so the final SELECT
                #    can succeed even if a TSV omitted optional fields.
                # --------------------------------------------------------------
                final_order = [f.name for f in schema.fields]
                missing = set(final_order) - set(df.columns)
                for col in missing:
                    dt = next((fld.dataType for fld in schema.fields if fld.name == col), StringType())
                    df = df.withColumn(col, F.lit(None).cast(dt))

                # --------------------------------------------------------------
                # 7) Re‑order columns to exactly match the CDM schema.
                # --------------------------------------------------------------
                df = df.select(*final_order)

                # --------------------------------------------------------------
                # 8) Apply the schema with metadata to preserve column comments
                #    Use nullable=True to handle incomplete TSV data
                # --------------------------------------------------------------
                df = _apply_schema_with_metadata(spark, df, schema, make_nullable=True)

            except Exception as exc:  # pragma: no cover
                logging.warning(
                    f"Could not load TSV for {coral_type_name} from {tsv_path}: {exc}"
                )
                # Fallback: create an empty DataFrame with the final schema
                df = spark.createDataFrame([], schema)
        else:
            # No TSV loading – create an empty DataFrame with the final schema
            df = spark.createDataFrame([], schema)

        # ------------------------------------------------------------------
        # Debug: show schema and a few rows before writing
        # ------------------------------------------------------------------
        logging.info(f"--- Schema for table `{db_name}.{table_name}` ---")
        df.printSchema()
        logging.info(f"--- Sample rows for `{db_name}.{table_name}` (up to 5) ---")
        df.show(5, truncate=False)

        # ------------------------------------------------------------------
        # Drop the table if it already exists (environment‑specific handling)
        # ------------------------------------------------------------------
        spark.sql(f"DROP TABLE IF EXISTS {db_name}.{table_name}")

        # ------------------------------------------------------------------
        # Write the Delta table (no .mode("overwrite") needed)
        # ------------------------------------------------------------------
        df.write.format(fmt).option("comment", table_comment).saveAsTable(
            f"{db_name}.{table_name}"
        )
        logging.info(f"Created table {db_name}.{table_name}")

    # ------------------------------------------------------------------
    # Auxiliary table: sys_typedef
    # ------------------------------------------------------------------
    if sys_oterm_df is None:
        # If we couldn't load sys_oterm earlier, try again now
        try:
            sys_oterm_df = spark.table(f"{db_name}.sys_oterm")
        except Exception as e:
            logging.error(f"Cannot create sys_typedef without sys_oterm table: {e}")
            raise
    
    sys_typedef_df = build_typedefs_dataframe(spark, all_typedef_rows, sys_oterm_df)

    logging.info(f"--- Schema for auxiliary table `{db_name}.sys_typedef` ---")
    sys_typedef_df.printSchema()
    logging.info(f"--- Sample rows for `{db_name}.sys_typedef` (up to 5) ---")
    sys_typedef_df.show(5, truncate=False)

    spark.sql(f"DROP TABLE IF EXISTS {db_name}.sys_typedef")
    sys_typedef_df.write.format(fmt).option(
        "comment", "CORAL type definitions"
    ).saveAsTable(f"{db_name}.sys_typedef")
    logging.info(f"Created auxiliary table {db_name}.sys_typedef")

    logging.info("✅ CDM generation complete.")


# ------------------------------------------------------------------
# Execute when run inside a notebook
# ------------------------------------------------------------------
spark = get_spark_session()               # <-- defined elsewhere
db_name = "enigma_coral"
spark.sql(f"USE {db_name}")
generate_cdm(spark, db_name)
